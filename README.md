# can-cache

<p align="center">
  <img src="https://img.shields.io/badge/quarkus-3.x-4695EB?logo=quarkus&logoColor=white" alt="Built with Quarkus 3" />
  <img src="https://img.shields.io/badge/protocol-cancached-orange" alt="Protocol compatible" />
  <img src="https://img.shields.io/badge/distribution-consistent%20hashing-4c1" alt="Consistent hashing" />
</p>

<p align="center">
  <a href="#turkish">TÃ¼rkÃ§e</a> Â· <a href="#english">English</a>
</p>

<a id="turkish"></a>

> **can-cache**, cancached metin protokolÃ¼ ile %100 uyumlu, Quarkus 3 tabanlÄ± hafif ama kÃ¼me Ã¶lÃ§ekli bir bellek iÃ§i anahtarâ€“deÄŸer sunucusudur. Tek JVM ile baÅŸlayÄ±p saniyeler iÃ§inde tutarlÄ± hash halkasÄ±na katÄ±lan, gecikmeye duyarlÄ± replikasyon ve anlÄ±k snapshot alma Ã¶zellikleriyle modern uygulamalarÄ±n cache katmanÄ±na gÃ¼Ã§ verir.

---

## Ä°Ã§indekiler
- [Animasyonlu YaÅŸam DÃ¶ngÃ¼sÃ¼](#animasyonlu-yaÅŸam-dÃ¶ngÃ¼sÃ¼)
- [Ã–ne Ã‡Ä±kan Ã–zellikler](#Ã¶ne-Ã§Ä±kan-Ã¶zellikler)
- [Neden can-cache?](#neden-can-cache)
- [Mimari Anahat](#mimari-anahat)
- [2 Dakikada Demo](#2-dakikada-demo)
- [KÃ¼me Kurulum Ã–rneÄŸi](#kÃ¼me-kurulum-Ã¶rneÄŸi)
- [YapÄ±landÄ±rma SihirbazÄ±](#yapÄ±landÄ±rma-sihirbazÄ±)
- [Proje YapÄ±sÄ±](#proje-yapÄ±sÄ±)
- [Yol HaritasÄ±](#yol-haritasÄ±)
- [KatkÄ± & Geri Bildirim](#katkÄ±--geri-bildirim)
- [Lisans](#lisans)

## Animasyonlu YaÅŸam DÃ¶ngÃ¼sÃ¼

<p align="center">
  <img src="docs/assets/cluster-lifecycle.svg" alt="Ä°ki node'lu can-cache veri yolculuÄŸu animasyonu" width="860" />
</p>

Animasyon, kÃ¼Ã§Ã¼k bir kÃ¼menin saniyeler iÃ§indeki yolculuÄŸunu Ã¼Ã§ ayrÄ± dÃ¶ngÃ¼ halinde gÃ¶sterir: **kÃ¼me geniÅŸlemesi**, **veri mutasyonlarÄ±** ve **topoloji daralmasÄ±**. BÃ¶ylece hem `set/add` gibi yazma hem de `delete` isteklerinin aynÄ± topolojik olaylar Ã¼zerinden nasÄ±l yÃ¶netildiÄŸini tek bakÄ±ÅŸta kavrayabilirsiniz.

### 1) KÃ¼me GeniÅŸlemesi â€” Node Ekleme DÃ¶ngÃ¼sÃ¼

1. **Node keÅŸfi:** Multicast heartbeat ile Node B tanÄ±nÄ±r ve tutarlÄ± hash halkasÄ±na eklenir.
2. **Bootstrap hazÄ±rlÄ±ÄŸÄ±:** Lider node, `SnapshotScheduler` tarafÄ±ndan Ã¼retilen son anlÄ±k gÃ¶rÃ¼ntÃ¼yÃ¼ ve `HintedHandoffService` Ã¼zerinde bekleyen ipuÃ§larÄ±nÄ± paylaÅŸmaya hazÄ±rlanÄ±r.
3. **Snapshot aktarÄ±mÄ±:** `ReplicationServer`, delta ve snapshot parÃ§alarÄ±nÄ± stream ederek yeni node'u sÄ±cak yedeÄŸe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
4. **Anti-entropy senkronizasyonu:** Hash halkasÄ± Ã¼zerindeki segmentler karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r; eksik kayÄ±tlar `RemoteNode` vekilleri ile tamamlanÄ±r.
5. **SaÄŸlÄ±k kontrolÃ¼:** Yeni Ã¼yenin kalp atÄ±ÅŸÄ± belirlenen periyot boyunca tutarlÄ± gelirse kÃ¼me replikasyon faktÃ¶rÃ¼nÃ¼ gÃ¼nceller.

Bu aÅŸamanÄ±n sonunda Node B hem yazma hem okuma trafiÄŸi alabilecek kadar gÃ¼ncel hale gelir ve animasyonda turuncu/yeÅŸil hatlarla iÅŸaretlenen mutasyon dÃ¶ngÃ¼sÃ¼ne katÄ±lÄ±r.

### 2) Veri Mutasyon DÃ¶ngÃ¼sÃ¼ â€” `set/add/delete`

KÃ¼meye katÄ±lan her node aynÄ± mutasyon akÄ±ÅŸÄ±nÄ± izler. Animasyondaki `set` isteÄŸi bunun sekiz karelik rotasÄ±nÄ± Ã¶zetler:

1. **Ä°stek kabulÃ¼:** `CanCachedServer` protokol satÄ±rÄ±nÄ± ayrÄ±ÅŸtÄ±rÄ±r ve `ClusterClient` Ã§aÄŸrÄ±sÄ±nÄ± hazÄ±rlar.
2. **Replika seÃ§imi:** Hash halkasÄ± lideri (Node A) ve takipÃ§iyi (Node B) belirler.
3. **Yerel yazÄ±m:** Lider `CacheEngine` segmentine yazar, TTL ve metrikler gÃ¼ncellenir.
4. **Uzak Ã§oÄŸaltma:** `RemoteNode` vekili komutu Node B `ReplicationServer`Ä±na aktarÄ±r.
5. **Quorum cevabÄ±:** Yeterli ACK sonrasÄ± `STORED` istemciye dÃ¶ner.
6. **Art alan:** TTL temizleme, hinted handoff ve anti-entropy dÃ¶ngÃ¼sÃ¼ sÃ¼rekli iÅŸler.

`add` komutu aynÄ± akÄ±ÅŸta sadece Ã¶n koÅŸul denetimi ekleyerek ilerler; anahtar zaten varsa `NOT_STORED` cevabÄ± verilir. `delete` komutunda ise Ã¼Ã§Ã¼ncÃ¼ adÄ±mda `CacheEngine.remove` tetiklenir, sonrasÄ±nda replikalara `D` Ã§erÃ§evesi gÃ¶nderilir ve quorum doÄŸrulamasÄ± `DELETED` yanÄ±tÄ± ile tamamlanÄ±r. Her iki durumda da art alan sÃ¼reÃ§leri (TTL sÃ¼pÃ¼rme, ipucu yeniden oynatma) tutarlÄ±lÄ±ÄŸÄ± korumak iÃ§in devrededir.

### 3) KÃ¼me DaralmasÄ± â€” Node Silme DÃ¶ngÃ¼sÃ¼

1. **Kalp atÄ±ÅŸÄ± kaybÄ±:** `CoordinationService` ardÄ±ÅŸÄ±k timeout'lar sonrasÄ± node'u ÅŸÃ¼pheli olarak iÅŸaretler.
2. **Hash halkasÄ±ndan Ã§Ä±karma:** Sanal dÃ¼ÄŸÃ¼mler kaldÄ±rÄ±lÄ±r; liderlik gÃ¶revleri kalan nodelar arasÄ±nda yeniden daÄŸÄ±tÄ±lÄ±r.
3. **Hint kuyruÄŸu doldurma:** UlaÅŸÄ±lamayan node'a gÃ¶nderilemeyen mutasyonlar `HintedHandoffService` kuyruÄŸuna alÄ±nÄ±r.
4. **Temizlik:** `CacheEngine` Ã¼zerindeki replika sayaÃ§larÄ± ve metrikler gÃ¼ncellenir, TTL kuyruÄŸu yeni sahiplik bilgisiyle hizalanÄ±r.
5. **Geri dÃ¶nÃ¼ÅŸ senaryosu:** Node yeniden Ã§evrimiÃ§i olursa bootstrap dÃ¶ngÃ¼sÃ¼ tetiklenir; aksi halde ipuÃ§larÄ± anti-entropy turunda kalÄ±cÄ± olarak temizlenir.

Bu dÃ¶ngÃ¼, animasyonda silinen node'un turuncudan griye dÃ¶nen kareleriyle gÃ¶sterilir ve `delete`/`touch` gibi komutlar sÄ±rasÄ±nda da veri gÃ¼venliÄŸini saÄŸlar.

## Ã–ne Ã‡Ä±kan Ã–zellikler

### âš¡ Protokol & Performans
- cancached metin protokolÃ¼nÃ¼n tÃ¼m Ã§ekirdek komutlarÄ±nÄ± (`set/add/replace/append/prepend/cas/get/gets/delete/incr/decr/touch/flush_all/stats/version/quit`) bire bir uygular, 1 MB Ã¼zerindeki yÃ¼kleri reddeder ve 30 gÃ¼nÃ¼ aÅŸan TTL deÄŸerlerini epoch olarak yorumlar.
- CAS sayaÃ§larÄ± atomik olarak Ã¼retilir; `StoredValueCodec` sayesinde CAS, bayrak ve TTL tek bir Base64 dizesinde taÅŸÄ±nÄ±r.
- SegmentlenmiÅŸ `CacheEngine` ile seÃ§ilebilir LRU ya da TinyLFU tahliye politikalarÄ±, milisaniye hassasiyetinde TTL temizliÄŸi ve yÃ¼ksek isabet oranÄ± saÄŸlar.

### ğŸ›¡ï¸ DayanÄ±klÄ±lÄ±k & TutarlÄ±lÄ±k
- Sanal dÃ¼ÄŸÃ¼m destekli **tutarlÄ± hash halkasÄ±** Ã¼zerinde Ã§alÄ±ÅŸan `ClusterClient`, replikasyon faktÃ¶rÃ¼ kadar kopyayÄ± deterministik biÃ§imde seÃ§er ve yazmalarÄ± Ã§oÄŸunluk quorum'una taÅŸÄ±r.
- `HintedHandoffService`, baÅŸarÄ±sÄ±z kopyalar iÃ§in ipuÃ§larÄ±nÄ± kalÄ±cÄ±laÅŸtÄ±rÄ±p node geri dÃ¶ndÃ¼ÄŸÃ¼nde otomatik oynatÄ±r; veri kayÄ±plarÄ±nÄ± en aza indirir.
- `SnapshotScheduler`, RDB benzeri dosya formatÄ±yla periyodik snapshot alÄ±r; uygulama yeniden baÅŸladÄ±ÄŸÄ±nda belleÄŸi aynÄ± dosyadan doldurur.

### ğŸ” GÃ¶zlemlenebilirlik & Operasyon
- `MetricsRegistry` + `MetricsReporter`, mikro saniye hassasiyetinde sayaÃ§ ve zamanlayÄ±cÄ± istatistiklerini periyodik olarak raporlar.
- `Broker` yayÄ±nla-abone ol modeliyle `keyspace:set` ve `keyspace:del` olaylarÄ±nÄ± servis eder; `CacheEngine.onRemoval` abonelikleri ile cache yaÅŸam dÃ¶ngÃ¼sÃ¼ izlenebilir.
- Multicast tabanlÄ± `CoordinationService`, yeni JVM Ã¶rneklerini otomatik keÅŸfeder, zaman aÅŸÄ±mÄ±na uÄŸrayan node'larÄ± temizler.

## Neden can-cache?
- **Ciddi Ã¼retim senaryolarÄ± iÃ§in tasarlandÄ±:** Gecikmeye duyarlÄ± replikasyon, hinted handoff ve anti-entropy dÃ¶ngÃ¼leri ile aÄŸ kesintilerini tolere eder.
- **Modern JVM Ã¶zelliklerinden faydalanÄ±r:** Sanal thread'ler, reaktif IO ve Quarkus ekosisteminin hÄ±zÄ±nÄ± kullanÄ±r.
- **Basit kurulum, hÄ±zlÄ± Ã¶lÃ§ekleme:** Tek bir komutla ayaÄŸa kalkar; yeni node'lar multicast ile kÃ¼meye otomatik katÄ±lÄ±r.
- **GeniÅŸletilebilir Ã§ekirdek:** Yeni codec'ler, tahliye stratejileri ve gÃ¶zlemleyiciler kolayca eklenebilir.

## Mimari Anahat

```mermaid
flowchart LR
    subgraph Client
        MC[cancached istemcisi]
    end
    MC -- TCP komutlarÄ± --> S[CanCachedServer]
    S -- yÃ¶nlendirme --> CC[ClusterClient]
    CC -- hash --> R[ConsistentHashRing]
    R -- yerel --> LN[(Yerel Node \nCacheEngine)]
    R -- uzak --> RN[(RemoteNode vekilleri)]
    RN --> RS[ReplicationServer]
    LN --> CE[CacheEngine]
    RS --> CE
    CE -- TTL temizliÄŸi --> DQ[DelayQueue]
    CE -- snapshot --> Snap[SnapshotFile/Scheduler]
    CE -- metrik/olay --> Obs[MetricsRegistry & Broker]
```

### Katmanlar
- **Komut iÅŸleme:** `CanCachedServer`, Quarkus ayaklandÄ±ÄŸÄ±nda konfigÃ¼re edilen portu dinler, satÄ±r bazlÄ± ayrÄ±ÅŸtÄ±rma yapar ve cancached protokolÃ¼nÃ¼n kenar durumlarÄ±nÄ± (CAS Ã§akÄ±ÅŸmasÄ±, `noreply`, `flush_all` gecikmesi vb.) bire bir uygular.
- **KÃ¼meleme:** `ConsistentHashRing`, `HashFn` implementasyonu ile sanal dÃ¼ÄŸÃ¼mler kullanÄ±r; `CoordinationService` multicast kalp atÄ±ÅŸlarÄ± ile Ã¼yeleri gÃ¼ncel tutar.
- **Replikasyon:** `RemoteNode` kÄ±sa Ã¶mÃ¼rlÃ¼ soketlerle `ReplicationServer`'a baÄŸlanÄ±r; `'S'/'G'/'D'/'X'/'C'` komutlarÄ±yla veri aktarÄ±mÄ± yaparken fingerprint karÅŸÄ±laÅŸtÄ±rmalarÄ± ile tutarlÄ±lÄ±ÄŸÄ± doÄŸrular.
- **Bellek motoru:** `CacheEngine`, segmentler, TTL kuyruÄŸu (`DelayQueue<ExpiringKey>`) ve CAS iÅŸlemlerini tek noktada yÃ¶netir; `AutoCloseable` aboneliklerle `curr_items` gibi istatistikler gÃ¼ncel tutulur.
- **KalÄ±cÄ±lÄ±k & gÃ¶zlemlenebilirlik:** `SnapshotFile` atomik dosya taÅŸÄ±mayla tutarlÄ±lÄ±ÄŸÄ± korur; `MetricsReporter` rapor periyodu > 0 olduÄŸunda saniyeler iÃ§inde metrikleri yazdÄ±rÄ±r.

## 2 Dakikada Demo

> Gereksinimler: Maven Wrapper (`./mvnw`) ve JDK 25.

```bash
# 1) GeliÅŸtirme modunda sunucuyu baÅŸlatÄ±n
./mvnw quarkus:dev

# 2) Temel bir doÄŸrulama yapÄ±n
printf 'set foo 0 5 3\r\nbar\r\nget foo\r\n' | nc 127.0.0.1 11211
# Beklenen Ã§Ä±ktÄ±: STORED / VALUE foo 0 3
```

Paketleme sonrasÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
./mvnw package
java -jar target/quarkus-app/quarkus-run.jar
```

## KÃ¼me Kurulum Ã–rneÄŸi

Tek JVM'den kÃ¼melenmiÅŸ bir yapÄ±ya geÃ§iÅŸ bu kadar kolay:

```bash
# VarsayÄ±lan node
./mvnw quarkus:dev

# Ä°kinci node (farklÄ± portlarla)
./mvnw quarkus:dev \
    -Dquarkus.http.port=0 \
    -Dapp.network.port=11212 \
    -Dapp.cluster.replication.port=18081 \
    -Dapp.cluster.discovery.node-id=node-b
```

Multicast koordinasyon, yeni node'u otomatik keÅŸfeder ve tutarlÄ± hash halkasÄ±na ekler. Yazmalar quorum tamamlanana kadar bekler; baÅŸarÄ±sÄ±z takipÃ§iler iÃ§in hinted handoff devreye girer.

## YapÄ±landÄ±rma SihirbazÄ±

`application.properties` altÄ±nda sizi bekleyen baÅŸlÄ±ca anahtarlar:

| Anahtar | AÃ§Ä±klama | VarsayÄ±lan |
| --- | --- | --- |
| `app.cache.segments` | Segment sayÄ±sÄ±; eÅŸzamanlÄ±lÄ±k/kapasite dengesini belirler. | 8 |
| `app.cache.max-capacity` | Toplam giriÅŸ sÄ±nÄ±rÄ±. | 10000 |
| `app.cache.cleaner-poll-millis` | TTL temizleyicisinin kuyruÄŸu yoklama aralÄ±ÄŸÄ± (ms). | 100 |
| `app.cache.eviction-policy` | `LRU` veya `TINY_LFU`. | LRU |
| `app.rdb.path` | Snapshot dosya yolu. | `data.rdb` |
| `app.rdb.snapshot-interval-seconds` | Snapshot periyodu; 0 yalnÄ±zca baÅŸlangÄ±Ã§ta. | 60 |
| `app.cluster.virtual-nodes` | Her fiziksel dÃ¼ÄŸÃ¼m iÃ§in sanal dÃ¼ÄŸÃ¼m sayÄ±sÄ±. | 64 |
| `app.cluster.replication-factor` | Anahtar baÅŸÄ±na kopya sayÄ±sÄ±. | 1 |
| `app.cluster.discovery.multicast-group/port` | Multicast koordinasyon adresi. | 230.0.0.1 / 45565 |
| `app.cluster.discovery.heartbeat-interval-millis` | Kalp atÄ±ÅŸÄ± aralÄ±ÄŸÄ±. | 5000 |
| `app.cluster.discovery.failure-timeout-millis` | Ãœye zaman aÅŸÄ±mÄ± eÅŸiÄŸi. | 15000 |
| `app.cluster.discovery.node-id` | Opsiyonel sabit dÃ¼ÄŸÃ¼m kimliÄŸi. | (boÅŸ) |
| `app.cluster.replication.bind-host/advertise-host/port` | Replikasyon sunucusu adres bilgileri. | 0.0.0.0 / 127.0.0.1 / 18080 |
| `app.cluster.replication.connect-timeout-millis` | Uzak dÃ¼ÄŸÃ¼me baÄŸlanma zaman aÅŸÄ±mÄ±. | 5000 |
| `app.cluster.coordination.hint-replay-interval-millis` | Hinted handoff kuyruÄŸu iÃ§in yeniden oynatma denemeleri arasÄ±ndaki minimum sÃ¼re. | 5000 |
| `app.cluster.coordination.anti-entropy-interval-millis` | Anti-entropy taramalarÄ±nÄ±n periyodu (ms). | 30000 |
| `app.network.host/port/backlog/worker-threads` | cancached TCP sunucusu ayarlarÄ±. | 0.0.0.0 / 11211 / 128 / 16 |
| `app.memcache.max-item-size-bytes` | Tek bir deÄŸerin saklanabileceÄŸi maksimum boyut (bayt). | 1048576 |
| `app.memcache.max-cas-retries` | BaÅŸarÄ±sÄ±z CAS iÅŸlemleri iÃ§in tekrar deneme sayÄ±sÄ±. | 16 |
| `app.metrics.report-interval-seconds` | Metrik raporlama periyodu; 0 devre dÄ±ÅŸÄ±. | 5 |

## Proje YapÄ±sÄ±

| Dizin | Ä°Ã§erik |
| --- | --- |
| `src/main/java/com/can/net` | cancached TCP sunucusu ve protokol ayrÄ±ÅŸtÄ±rÄ±cÄ±larÄ±. |
| `src/main/java/com/can/cluster` | TutarlÄ± hash halkasÄ±, kÃ¼me istemcisi ve node arayÃ¼zleri. |
| `src/main/java/com/can/cluster/coordination` | Multicast koordinasyonu, uzak node vekilleri ve replikasyon sunucusu. |
| `src/main/java/com/can/core` | Ã–nbellek motoru, segmentler, TTL kuyruÄŸu ve tahliye politikalarÄ±. |
| `src/main/java/com/can/codec` | Anahtar/deÄŸer codec implementasyonlarÄ± (UTF-8, Java Serializable). |
| `src/main/java/com/can/rdb` | Snapshot dosyasÄ± ve zamanlayÄ±cÄ± bileÅŸenleri. |
| `src/main/java/com/can/metric` | SayaÃ§, zamanlayÄ±cÄ± ve konsol raporlayÄ±cÄ±sÄ±. |
| `src/main/java/com/can/pubsub` | Uygulama iÃ§i yayÄ±nla-abone ol altyapÄ±sÄ±. |
| `src/main/java/com/can/config` | CDI yapÄ±landÄ±rmasÄ± ve tip gÃ¼venli konfigÃ¼rasyon arayÃ¼zleri. |
| `integration-tests/` | Docker Compose ile Ã§alÄ±ÅŸan uÃ§tan uca cancached uyumluluk testleri. |
| `performance-tests/` | JMeter planlarÄ± ve NFR dokÃ¼manlarÄ±. |
| `scripts/` | YardÄ±mcÄ± komut dosyalarÄ± (`run-integration-tests.sh` vb.). |

## Yol HaritasÄ±

- [ ] Ek replikasyon stratejileri (Ã¶rn. aktif-aktif senaryolar iÃ§in CRDT araÅŸtÄ±rmasÄ±)
- [ ] Opsiyonel REST/HTTP yÃ¶netim ucu
- [ ] Prometheus metrik ihracÄ±
- [ ] Otomatik benchmark pipeline'Ä± (JMeter + GitHub Actions)
- [ ] Helm chart ile Kubernetes daÄŸÄ±tÄ±mÄ±

> Fikirlerin mi var? [Issue aÃ§](../../issues) veya PR gÃ¶nder!

## KatkÄ± & Geri Bildirim

1. Depoyu forklayÄ±n ve `main` Ã¼zerine deÄŸiÅŸikliklerinizi rebase edin.
2. Kod stilini koruyarak anlamlÄ± commit mesajlarÄ± yazÄ±n.
3. `./mvnw test` ve gerekiyorsa `./scripts/run-integration-tests.sh` ile doÄŸrulayÄ±n.
4. Deneyimlerinizi, performans Ã¶lÃ§Ã¼mlerinizi veya yeni kullanÄ±m senaryolarÄ±nÄ±zÄ± paylaÅŸÄ±n â€” proje bu geri bildirimlerle bÃ¼yÃ¼yor.

SorularÄ±nÄ±z mÄ± var? Bir [issue](../../issues/new) aÃ§abilir veya doÄŸrudan Pull Request ile gelebilirsiniz.

## Lisans

Bu proje, MIT LisansÄ± veya Apache LisansÄ± SÃ¼rÃ¼m 2.0 koÅŸullarÄ± altÄ±nda Ã§ift lisanslanmÄ±ÅŸtÄ±r. AyrÄ±ntÄ±lar iÃ§in [LICENSE-MIT](./LICENSE-MIT) ve [LICENSE-APACHE](./LICENSE-APACHE) dosyalarÄ±na gÃ¶z atabilirsiniz.

Bu depoya katkÄ±da bulunan herkes, katkÄ±larÄ±nÄ±n her iki lisansÄ±n koÅŸullarÄ± altÄ±nda da kullanÄ±labileceÄŸini kabul eder.

---

<a id="english"></a>
# can-cache (English)

<p align="center">
  <img src="https://img.shields.io/badge/quarkus-3.x-4695EB?logo=quarkus&logoColor=white" alt="Built with Quarkus 3" />
  <img src="https://img.shields.io/badge/protocol-cancached-orange" alt="Protocol compatible" />
  <img src="https://img.shields.io/badge/distribution-consistent%20hashing-4c1" alt="Consistent hashing" />
</p>

> **can-cache** is a lightweight yet cluster-scale in-memory keyâ€“value server built on Quarkus 3 that is 100% compatible with the cancached text protocol. It can boot on a single JVM and join the consistent hash ring within seconds, empowering the cache tier of modern applications with latency-aware replication and instant snapshots.

---

## Table of Contents
- [Animated Lifecycle](#animated-lifecycle)
- [Highlights](#highlights)
- [Why can-cache?](#why-can-cache)
- [Architecture Outline](#architecture-outline)
- [Demo in 2 Minutes](#demo-in-2-minutes)
- [Cluster Setup Example](#cluster-setup-example)
- [Configuration Wizard](#configuration-wizard)
- [Project Layout](#project-layout)
- [Roadmap](#roadmap)
- [Contributing & Feedback](#contributing--feedback)
- [License](#license)

## Animated Lifecycle

<p align="center">
  <img src="docs/assets/cluster-lifecycle.svg" alt="Animated journey of can-cache data across two nodes" width="860" />
</p>

The animation presents the journey of a small cluster within seconds across three different loops: **cluster expansion**, **data mutations**, and **topology shrinkage**. This way you can observe how both write operations such as `set/add` and deletions are managed on top of the very same topological events at a glance.

### 1) Cluster Expansion â€” Node Join Loop

1. **Node discovery:** Node B is identified through multicast heartbeats and added to the consistent hash ring.
2. **Bootstrap preparation:** The leader node gets ready to share the latest snapshot produced by `SnapshotScheduler` along with the hints waiting on `HintedHandoffService`.
3. **Snapshot transfer:** `ReplicationServer` streams delta and snapshot chunks to turn the newcomer into a hot standby.
4. **Anti-entropy synchronization:** Segments on the hash ring are compared; missing records are filled through `RemoteNode` proxies.
5. **Health check:** If the new member keeps a steady heartbeat for the configured period, the cluster updates its replication factor.

At the end of this phase Node B becomes up to date enough to receive both read and write traffic, joining the mutation loop highlighted with orange/green strokes in the animation.

### 2) Data Mutation Loop â€” `set/add/delete`

Every node that joins the cluster follows the same mutation pipeline. The `set` request in the animation summarises its eight-frame path:

1. **Request admission:** `CanCachedServer` parses the protocol line and prepares the `ClusterClient` call.
2. **Replica selection:** The hash ring picks the leader (Node A) and the follower (Node B).
3. **Local write:** The leader writes to its `CacheEngine` segment while refreshing TTL and metrics.
4. **Remote replication:** The `RemoteNode` proxy forwards the command to Node B's `ReplicationServer`.
5. **Quorum response:** After receiving enough ACKs the server returns `STORED` to the client.
6. **Background maintenance:** TTL sweeping, hinted handoff and anti-entropy loops continue to run.

The `add` command follows the same flow with an extra precondition check; if the key already exists the response is `NOT_STORED`. For `delete` the third step triggers `CacheEngine.remove`, afterwards replicas receive a `D` frame and the quorum verification concludes with a `DELETED` reply. In both cases the background processes (TTL sweeping, replaying hints) remain active to preserve consistency.

### 3) Cluster Shrink â€” Node Removal Loop

1. **Heartbeat loss:** `CoordinationService` marks the node as suspect after consecutive timeouts.
2. **Removing from the hash ring:** Virtual nodes are dropped and leadership duties are redistributed across the remaining nodes.
3. **Hint queue fill:** Mutations that cannot reach the missing node are stored in the `HintedHandoffService` queue.
4. **Cleanup:** Replica counters and metrics on `CacheEngine` are updated, and the TTL queue realigns with the new ownership.
5. **Return scenario:** If the node comes back online the bootstrap loop is triggered; otherwise the hints are purged during the next anti-entropy round.

The animation shows this loop with the removed node fading from orange to grey, ensuring data safety even during `delete`/`touch` commands.

## Highlights

### âš¡ Protocol & Performance
- Implements every core command of the cancached text protocol (`set/add/replace/append/prepend/cas/get/gets/delete/incr/decr/touch/flush_all/stats/version/quit`), rejects payloads larger than 1 MB, and interprets TTL values over 30 days as epoch timestamps.
- CAS counters are produced atomically; thanks to `StoredValueCodec` CAS, flags, and TTL travel in a single Base64 string.
- Segmented `CacheEngine` enables switchable LRU or TinyLFU eviction policies, millisecond-precision TTL cleanup and high hit rates.

### ğŸ›¡ï¸ Durability & Consistency
- `ClusterClient` operates on a **consistent hash ring** with virtual nodes, deterministically picking as many replicas as the replication factor and carrying writes to a majority quorum.
- `HintedHandoffService` persists hints for failed replicas and replays them when the node returns, minimising data loss.
- `SnapshotScheduler` takes periodic snapshots with an RDB-like file format and warms up memory from the same file when the application restarts.

### ğŸ” Observability & Operations
- `MetricsRegistry` + `MetricsReporter` periodically emit counter and timer statistics with microsecond precision.
- The `Broker` publish-subscribe model serves `keyspace:set` and `keyspace:del` events; `CacheEngine.onRemoval` subscribers make the cache lifecycle observable.
- Multicast-based `CoordinationService` discovers new JVM instances automatically and cleans up nodes that time out.

## Why can-cache?
- **Built for serious production scenarios:** Latency-aware replication, hinted handoff, and anti-entropy loops tolerate network partitions.
- **Leverages modern JVM capabilities:** Makes use of virtual threads, reactive IO, and the speed of the Quarkus ecosystem.
- **Simple to deploy, quick to scale:** Comes online with a single command; new nodes join the cluster automatically via multicast.
- **Extensible core:** New codecs, eviction strategies, and observers can be added with ease.

## Architecture Outline

```mermaid
flowchart LR
    subgraph Client
        MC[cancached client]
    end
    MC -- TCP commands --> S[CanCachedServer]
    S -- routing --> CC[ClusterClient]
    CC -- hash --> R[ConsistentHashRing]
    R -- local --> LN[(Local Node \nCacheEngine)]
    R -- remote --> RN[(RemoteNode proxies)]
    RN --> RS[ReplicationServer]
    LN --> CE[CacheEngine]
    RS --> CE
    CE -- TTL cleanup --> DQ[DelayQueue]
    CE -- snapshot --> Snap[SnapshotFile/Scheduler]
    CE -- metrics/events --> Obs[MetricsRegistry & Broker]
```

### Layers
- **Command processing:** `CanCachedServer` listens on the configured port once Quarkus boots, performs line-based parsing and mirrors the edge cases of the cancached protocol (CAS conflicts, `noreply`, `flush_all` delay, etc.).
- **Clustering:** `ConsistentHashRing` uses a `HashFn` implementation with virtual nodes; `CoordinationService` keeps the membership updated via multicast heartbeats.
- **Replication:** `RemoteNode` connects to `ReplicationServer` with short-lived sockets; `'S'/'G'/'D'/'X'/'C'` commands transfer data while verifying consistency through fingerprint comparisons.
- **In-memory engine:** `CacheEngine` manages segments, the TTL queue (`DelayQueue<ExpiringKey>`), and CAS operations; `AutoCloseable` subscriptions keep stats such as `curr_items` up to date.
- **Persistence & observability:** `SnapshotFile` relies on atomic file moves to stay consistent; `MetricsReporter` prints metrics within seconds when the report period is greater than zero.

## Demo in 2 Minutes

> Requirements: Maven Wrapper (`./mvnw`) and JDK 25.

```bash
# 1) Start the server in development mode
./mvnw quarkus:dev

# 2) Perform a quick validation
printf 'set foo 0 5 3\r\nbar\r\nget foo\r\n' | nc 127.0.0.1 11211
# Expected output: STORED / VALUE foo 0 3
```

To run after packaging:

```bash
./mvnw package
java -jar target/quarkus-app/quarkus-run.jar
```

## Cluster Setup Example

Scaling from a single JVM to a clustered topology is this easy:

```bash
# Default node
./mvnw quarkus:dev

# Second node (with alternate ports)
./mvnw quarkus:dev \
    -Dquarkus.http.port=0 \
    -Dapp.network.port=11212 \
    -Dapp.cluster.replication.port=18081 \
    -Dapp.cluster.discovery.node-id=node-b
```

Multicast coordination automatically discovers the new node and adds it to the consistent hash ring. Writes wait until the quorum completes; hinted handoff takes over for failed followers.

## Configuration Wizard

The most important keys waiting under `application.properties`:

| Key | Description | Default |
| --- | --- | --- |
| `app.cache.segments` | Number of segments; controls the balance between concurrency and capacity. | 8 |
| `app.cache.max-capacity` | Maximum number of entries. | 10000 |
| `app.cache.cleaner-poll-millis` | Poll interval (ms) for the TTL cleaner queue. | 100 |
| `app.cache.eviction-policy` | `LRU` or `TINY_LFU`. | LRU |
| `app.rdb.path` | Snapshot file path. | `data.rdb` |
| `app.rdb.snapshot-interval-seconds` | Snapshot period; 0 means only on startup. | 60 |
| `app.cluster.virtual-nodes` | Number of virtual nodes per physical node. | 64 |
| `app.cluster.replication-factor` | Number of replicas per key. | 1 |
| `app.cluster.discovery.multicast-group/port` | Multicast coordination address. | 230.0.0.1 / 45565 |
| `app.cluster.discovery.heartbeat-interval-millis` | Heartbeat interval. | 5000 |
| `app.cluster.discovery.failure-timeout-millis` | Member timeout threshold. | 15000 |
| `app.cluster.discovery.node-id` | Optional static node identifier. | (empty) |
| `app.cluster.replication.bind-host/advertise-host/port` | Address details for the replication server. | 0.0.0.0 / 127.0.0.1 / 18080 |
| `app.cluster.replication.connect-timeout-millis` | Connection timeout for remote nodes. | 5000 |
| `app.cluster.coordination.hint-replay-interval-millis` | Minimum delay between hint replay attempts. | 5000 |
| `app.cluster.coordination.anti-entropy-interval-millis` | Period (ms) for anti-entropy sweeps. | 30000 |
| `app.network.host/port/backlog/worker-threads` | Settings for the cancached TCP server. | 0.0.0.0 / 11211 / 128 / 16 |
| `app.memcache.max-item-size-bytes` | Maximum size (bytes) for a single value. | 1048576 |
| `app.memcache.max-cas-retries` | Retry count for failed CAS operations. | 16 |
| `app.metrics.report-interval-seconds` | Metrics reporting period; 0 disables the reporter. | 5 |

## Project Layout

| Directory | Contents |
| --- | --- |
| `src/main/java/com/can/net` | cancached TCP server and protocol parsers. |
| `src/main/java/com/can/cluster` | Consistent hash ring, cluster client, and node interfaces. |
| `src/main/java/com/can/cluster/coordination` | Multicast coordination, remote node proxies, and replication server. |
| `src/main/java/com/can/core` | Cache engine, segments, TTL queue, and eviction policies. |
| `src/main/java/com/can/codec` | Key/value codec implementations (UTF-8, Java Serializable). |
| `src/main/java/com/can/rdb` | Snapshot file and scheduler components. |
| `src/main/java/com/can/metric` | Counters, timers, and console reporter. |
| `src/main/java/com/can/pubsub` | In-process publish/subscribe infrastructure. |
| `src/main/java/com/can/config` | CDI configuration and type-safe configuration interfaces. |
| `integration-tests/` | End-to-end cancached compatibility tests powered by Docker Compose. |
| `performance-tests/` | JMeter plans and NFR documentation. |
| `scripts/` | Helper scripts (e.g. `run-integration-tests.sh`). |

## Roadmap

- [ ] Additional replication strategies (e.g., CRDT research for active-active scenarios)
- [ ] Optional REST/HTTP management endpoint
- [ ] Prometheus metrics exporter
- [ ] Automated benchmarking pipeline (JMeter + GitHub Actions)
- [ ] Kubernetes deployment via Helm chart

> Got ideas? [Open an issue](../../issues) or send a PR!

## Contributing & Feedback

1. Fork the repository and rebase your changes onto `main`.
2. Follow the existing code style and write meaningful commit messages.
3. Verify with `./mvnw test` and, if necessary, `./scripts/run-integration-tests.sh`.
4. Share your experiences, performance measurements, or new use cases â€” the project grows with community feedback.

Have questions? [Open an issue](../../issues/new) or come directly with a Pull Request.

## License

This project is dual-licensed under the terms of the MIT License or the Apache License Version 2.0. See [LICENSE-MIT](./LICENSE-MIT) and [LICENSE-APACHE](./LICENSE-APACHE) for details.

Everyone contributing to this repository agrees that their contributions can be used under both licenses.
